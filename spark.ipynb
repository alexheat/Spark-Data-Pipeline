{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, MapType, ArrayType, BooleanType\n",
    "#Convert data Unix timestamp to datetime\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import desc\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "game_api_raw = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"events\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "game_api_raw.cache()\n",
    "game_api_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_api_raw.printSchema()\n",
    "game_api_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the 'value' field and cast it as a string \n",
    "game_api_string = game_api_raw.select(game_api_raw.value.cast('string'))\n",
    "game_api_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_api_df = game_api_string.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "\n",
    "game_api_df.printSchema()\n",
    "game_api_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Save all of the responses with all collumns to a parque file\n",
    "game_api_df.write.mode(\"overwrite\").parquet(\"/tmp/game/all_api_requests\")\n",
    "\n",
    "#Drop the collum with the request headers as they are not need for the downstream analysis \n",
    "game_api_df_light = game_api_df.drop('request_headers')\n",
    "\n",
    "#Save the purchases and sales to seperate files for analysis \n",
    "game_api_df_light.filter(game_api_df_light.event_type==\"sell_item\").write.mode(\"overwrite\").parquet(\"/tmp/game/sell_api\")\n",
    "game_api_df_light.filter(game_api_df_light.event_type==\"purchase_item\").write.mode(\"overwrite\").parquet(\"/tmp/game/purchase_api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/tmp/game/purchase_api 4096\n",
      "file:/tmp/game/all_api_requests 4096\n",
      "file:/tmp/game/sell_api 4096\n"
     ]
    }
   ],
   "source": [
    "#Create a function that can be used to list the contents of a directory in Hadoop\n",
    "import sys\n",
    "def list_hadoop_files(path):\n",
    "    hadoop = sc._jvm.org.apache.hadoop\n",
    "    fs = hadoop.fs.FileSystem\n",
    "    conf = hadoop.conf.Configuration() \n",
    "    fs_path = hadoop.fs.Path(path)\n",
    "\n",
    "    try:\n",
    "        for f in fs.get(conf).listStatus(fs_path):\n",
    "            print(f.getPath(), f.getLen())\n",
    "    except:\n",
    "        print(\"Path '{}' does not exist.\".format(path))\n",
    "\n",
    "list_hadoop_files('/tmp/game/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Read events from kafka\n",
    "game_api_raw = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"events\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "game_api_raw.cache()\n",
    "\n",
    "#Select the 'value' collumn as a string so it can be converted to a json object \n",
    "game_api_string = game_api_raw.select(game_api_raw.value.cast('string'))\n",
    "game_api_df = game_api_string.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "\n",
    "#Save all of the responses with all collumns to a parque file\n",
    "game_api_df.write.mode(\"overwrite\").parquet(\"/tmp/game/all_api_requests\")\n",
    "\n",
    "#Drop the collum with the request headers as they are not need for the downstream analysis \n",
    "game_api_df_light = game_api_df.drop('request_headers')\n",
    "\n",
    "#Save the purchases and sales to seperate files for analysis \n",
    "game_api_df_light.filter(game_api_df_light.event_type==\"sell_item\").write.mode(\"overwrite\").parquet(\"/tmp/game/sell_api\")\n",
    "game_api_df_light.filter(game_api_df_light.event_type==\"purchase_item\").write.mode(\"overwrite\").parquet(\"/tmp/game/purchase_api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sc._jvm.org.apache.hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
